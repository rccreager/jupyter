{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rachael Creager\n",
    "\n",
    "20 May 2018\n",
    "\n",
    "This notebook will be used to guide the UPenn physics machine learning journal club discussion for the 21st.\n",
    "\n",
    "Paper for discussion: \n",
    "* [Identifying the relevant dependencies of the neural network\n",
    "response on characteristics of the input space](https://arxiv.org/pdf/1803.08782.pdf)\n",
    "\n",
    "Related interesting references: \n",
    "* [Explaining NonLinear Classification Decisions with\n",
    "Deep Taylor Decomposition](https://arxiv.org/pdf/1512.02479.pdf)\n",
    "* [DeepTaylor talk](http://heatmapping.org/deeptaylor/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Section 1\n",
    "### Introduction\n",
    "\n",
    "* A neural network is multi-parameter classifier, parameters determined at training time.\n",
    "* In many applications, training sample and testing sample are not guaranteed to be congruent/similar\n",
    "* To make sure we understand the model's behavior, we want to identify the NN inputs with the largest influence on the output\n",
    "* In this paper, this effect will be analyzed by calculating a (2nd order) Taylor expansion of the NN function with respect to each input variable\n",
    "* Reminder: [Taylor Decomposition](http://www.math.ucdenver.edu/~esulliva/Calculus3/Taylor.pdf)\n",
    "    * Let f be an infinitely differentiable function (x,y) -> R in an open neighborhood around (a,b):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{equation}\n",
       "f(x,y) = f(a,b) + f_x(a,b)(x-a) + f_y(a,b)(y-b) + \\frac{1}{2!}[f_{xx}(a,b)(x-a)^2 + 2f_{xy}(a,b)(x-a)(y-b) + f_{yy}(y-b)^2]+...\n",
       "\\end{equation}"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Latex(r\"\"\"\\begin{equation}\n",
    "f(x,y) = f(a,b) + f_x(a,b)(x-a) + f_y(a,b)(y-b) + \\frac{1}{2!}[f_{xx}(a,b)(x-a)^2 + 2f_{xy}(a,b)(x-a)(y-b) + f_{yy}(y-b)^2]+...\n",
    "\\end{equation}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Consider two types of features: \n",
    "    * First order: those from first-order Taylor coeffs. Should capture influence of single input elements\n",
    "    * Second order: those from second-order Taylor coeffs. Should capture pairwise correlations or self-correlations (autocorrelation)\n",
    "* Depending on the task, the influence of a given feature could vary over the input space\n",
    "    * To deal with this, the metric used to measure influence of a feature is arithmetic mean of abs value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{eqnarray}\n",
       "<t_i> = \\frac{1}{N} \\sum_{j=1}^{N} |t_i (\\{ x_j\\})|\n",
       "\\end{eqnarray}"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Latex(r\"\"\"\\begin{eqnarray}\n",
    "<t_i> = \\frac{1}{N} \\sum_{j=1}^{N} |t_i (\\{ x_j\\})|\n",
    "\\end{eqnarray}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $t_i$ corresponds to the Taylor coefficienct and $\\{x_j\\}$ is the set of input elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "* Can we guarantee a limit on the higher-order terms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: \n",
    "### Analysis of features of the input space for simple tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with an easy example :)\n",
    "\n",
    "Consider a binary classification task with a two-dimensional input, $x_1$ and $x_2$. \n",
    "We'll consider four different tasks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"table.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The signal and background are Gaussian distributions with the $(x_1,x_2)$ columns describing the center (i.e. $(x_1,x_2) = 0.5$ means the center of the distribution is $(0.5,0.5)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"plots.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For 1a, the signal and background are shifted from each other. $x_1$ and $x_2$ are uncorrelated and equally spread. \n",
    "    * In this case, $<t_1>$ and $<t_2>$ have high values, indicating that you get strong separation power from the marginal probability distributions of $x_1$ and $x_2$\n",
    "    * You also get a non-zero correlation between $x_1$ and $x_2$ because of the relative position of the two distributions\n",
    "* For 1b, the signal and background have the same center. $x_1$ and $x_2$ are equally spread, but with different correlations for signal and background\n",
    "    * The first order features $x_1$ and $x_2$ are somewhat useful for classification -- around the origin, they are useless, but for large absolute values they give a bit of separation power (we see this since $<t_{x_1}>$ and $<t_{x_2}>$ are somewhat large)\n",
    "    * Since the correlation is the key difference between the distributions here, we expect $<t_{x_1,x_2}>$ to be the largest term -- and we see this in the results!\n",
    "* 1c is a mixture of cases 1a and 1b\n",
    "* For 1d, the distributions have the same center. $x_1$ and $x_2$ are uncorrelated, but signal is more tightly \"spread\" than background\n",
    "    * In this case, we would expect the first order terms to help in the case of large absolute values\n",
    "    * Self-correlation might help as well, to try to get a handle on the \"peakiness\" of the distribution\n",
    "    * The relationship between $x_1$ and $x_2$ won't help much, since they are uncorrelated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3\n",
    "### Analysis of the learning progress\n",
    "\n",
    "* We will now consider task 1c to observe how $<t_i>$ may be used as a metric for learning\n",
    "* This evaluation will be done by observing how the area under the curve (AUC) changes for the receiver operating characteristic (ROC) curve\n",
    "    * Reminder: [ROC curve](http://gim.unmc.edu/dxtests/roc3.htm)\n",
    "    * A ROC curve is a way of measuring the \"goodness\" of a classifier\n",
    "    * A \"perfect\" curve has an AUC of 1\n",
    "    * AUC <= 0.5 is completely garbage!\n",
    "    \n",
    "<img src=\"roc.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we evaluate training over 350 iterations with AUC, and measure the relative contribution of the first- and second-order features using $<t_i>$\n",
    "* In steps 0-30, we see AUC rise quickly, plateauing around 0.84\n",
    "    * We can see the values of $<t_{x_1}>$, $<t_{x_2}>$ get large. This means the classifier quickly learned to use the first-order features to improve its performance. \n",
    "    * We also see a contribution from $<t_{x_1,x_2}>$, and a minor contributions from the autocorrelation terms\n",
    "* After the plateau (approx. iterations 50-100), we see a gradual rise in AUC by another 0.03 or so, reaching a final plateau around 0.85\n",
    "    * In this second increase, it appears to be giving more influence to the second-order terms and decreasing the influence of the first-order terms\n",
    "\n",
    "<img src=\"training.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4\n",
    "### Application to a benchmark task from high-energy particle physics\n",
    "\n",
    "Now let's test it on a trickier problem, the Higgs boson machine learning challenge dataset ([kaggle link](https://www.kaggle.com/c/higgs-boson/data))\n",
    "* Searching for $H \\rightarrow \\tau^+ \\tau^-$\n",
    "* $\\tau$ particles are notoriously difficult to identify/measure\n",
    "* No \"obvious\" physics signature to distinguish $\\tau$'s from Higgs decays versus other decays\n",
    "These features make this problem particularly well-suited to NN's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A full list of the 30 input variables is available [here, on pages 14-16](https://higgsml.lal.in2p3.fr/files/2014/04/documentation_v1.8.pdf).\n",
    "* The **PRI_** quantities are \"primary\" variables, i.e. directly calculated from measurements\n",
    "    * Some of these quantities (e.g. $\\phi$) will provide no separation power due to symmetries of the problem\n",
    "* The **DER_** quantities are derived using primary quantities\n",
    "* In general, we expect the derived quantities to be more useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The authors trained a simple NN using all 30 input features. The trained classifier gave AUC = 0.92 with approx. median significance of 2.61. The 30 input features result in 295 first- and second-order Taylor coefficient features.\n",
    "\n",
    "To evaluate the influence of each variable, the metrics $<t_i>$ are ranked:\n",
    "<img src=\"metrics_ranked.png\">\n",
    "* The most important variable is the invariant mass of the hadronic $\\tau$ plus lepton\n",
    "    * Among first-order terms, it is the most important (ranked 10th)\n",
    "    * Among important second-order terms, it appears in six of them\n",
    "    * Using these, the authors determine that the NN is learning the tau invariant mass peak position and width\n",
    "    * By this metric, the most important single term comes from correlation of the $\\tau$+lepton mass and the ratio $\\frac{lepton_{p_T}}{\\tau_{p_T}}$\n",
    "* As expected, terms such as azimuthasl angle $\\phi$ had very little influence\n",
    "* The authors tried re-training the algorithm using only the inputs contributing to the top 5% of $<t_i>$ terms\n",
    "    * This reduced the list of inputs to 8 variables\n",
    "    * Achieved similar/identical performance\n",
    "    * Authors did nor perform deeper analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusions:\n",
    "* $<t_i>$ is an interesting metric of NN performance\n",
    "* Using some toy cases, we see the behavior we expect when using this metric to evaluate importance of various input terms\n",
    "* Using a more complicated case, some obvious physics interpretations could be made\n",
    "\n",
    "Rachael's questions/concerns:\n",
    "* Do we really want to consider the entire space equally? Shouldn't some parts of the input space (i.e. your signal region versus control region) be given more \"influence?\n",
    "* What are other metrics of Taylor coefficient \"influence\"?\n",
    "* What about higher-order terms? Big-oh/little-oh constraints on higher-order terms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
